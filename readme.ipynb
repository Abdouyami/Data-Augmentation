{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Report: Data Augmentation for Employee Chatbot**\n",
    "\n",
    "#### **1. Introduction**  \n",
    "\n",
    "The objective of this project was to enhance a dataset of employee queries to improve the performance of a chatbot. The original dataset consisted of **1,022 rows** with three columns: `user_query`, `intent`, and `solution`.  \n",
    "\n",
    "After evaluating multiple data augmentation techniques, the **GPT-4o Mini model** (**Rogue-Rose-103b-v0.2**, via OpenRouter API) was selected for its ability to generate high-quality and diverse queries. Additionally, the **Back Translation technique** was employed to further enrich the dataset.  \n",
    "\n",
    "The final augmented dataset was divided into three versions:  \n",
    "- **Version 1**: `13,225 rows`  \n",
    "- **Version 2**: `14,165 rows`  \n",
    "- **Version 3**: `14,952 rows`  \n",
    "\n",
    "To support a **French-language chatbot**, the dataset was translated into French using the `deep-translator` library. This process ensured that the dataset is available in both **English and French**, making it suitable for multilingual chatbot training.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Techniques Evaluated**\n",
    "Several techniques were initially explored for data augmentation, but only **GPT-4o Mini** was used for the final workflow due to its superior performance.\n",
    "\n",
    "##### **2.1 Paraphrasing**\n",
    "- **Objective**: Generate variations of existing queries while preserving the `intent` and `solution`.\n",
    "- **Tools Used**: `nlpaug` library with BERT-based paraphrasing.\n",
    "- **Challenges**:\n",
    "  - **Grammar Issues**: Some paraphrased queries had grammatical errors (e.g., \"I are having trouble\").\n",
    "  - **Irrelevant Content**: Occasionally, the paraphrased queries were nonsensical or irrelevant.\n",
    "- **Solutions**:\n",
    "  - Integrated *grammar correction* using language-tool-python.\n",
    "  - Added *validation* to filter out irrelevant outputs.\n",
    "- **Results**:\n",
    "  - Generated **8,176 rows** across 3 attempts (75 minutes per attempt).\n",
    "  - **Not used in the final dataset** due to quality issues.\n",
    "\n",
    "##### **2.2 Back-Translation**\n",
    "- **Objective**: Translate queries into an intermediate language (e.g., French) and back to English to generate diverse variations.\n",
    "- **Tools Used**: `deep-translator` library\n",
    "- **Results**:\n",
    "  - Generated **1022 rows** in 32 minutes.\n",
    "\n",
    "##### **2.3 Synthetic Data Generation**\n",
    "- **Objective**: Generate new queries based on the `intent` and `solution` using advanced language models.\n",
    "- **Tools Used**:\n",
    "  - *GPT-2*: Initial attempts with GPT-2 produced generic or irrelevant outputs.\n",
    "  - *T5*: Improved results with T5, but some outputs were still repetitive or nonsensical.\n",
    "  - *OpenRouter API*:\n",
    "    - *DeepSeek-R1*: Produced high-quality outputs but limited diversity.\n",
    "    - *GPT-4o Mini*: Achieved the best balance of quality and diversity.\n",
    "    - *Rogue-Rose-103b-v0.2*: Achieved the best balance of quality and diversity\n",
    "- **Challenges**:\n",
    "  - *Irrelevant Outputs*: GPT-2 and T5 sometimes generated irrelevant or repetitive queries.\n",
    "  - *API Limitations*: OpenRouter API had occasional rate limits or errors.\n",
    "- **Solutions**:\n",
    "  - Refined *prompts* to guide the model better.\n",
    "  - Added *validation* to filter out irrelevant outputs.\n",
    "  - Switched to *GPT-4o Mini* and *Rogue-Rose-103b-v0.2* for better performance and diversity.\n",
    "- **Results**:\n",
    "    1. *GPT-4o Min*\n",
    "        - Generated **13,798 rows** in 140 minutes.\n",
    "        - Achieved the best balance of **quality** and **diversity**.\n",
    "        - **Selected for the final dataset**.\n",
    "    2. *Rogue-Rose-103b-v0.2*\n",
    "        - Generated **961 rows** in 39 minutes.\n",
    "        - Achieved the best balance of **quality** and **diversity**.\n",
    "        - **Selected for the final dataset**. \n",
    "---\n",
    "\n",
    "#### **3. Final Workflow: GPT-4o Mini/ Rogue Rose**\n",
    "The final workflow exclusively used the **GPT-4o Mini model** and **Rogue Rose-103b-v0.2 model** for data augmentation. Here’s how it was implemented:\n",
    "\n",
    "##### **3.1 Setup**\n",
    "1. **API Integration**:\n",
    "   - Used the OpenRouter API to access the GPT-4o Mini model and Rogue Rose-103b-v0.2 model.\n",
    "   - Initialized the OpenAI client with the OpenRouter base URL and API key.\n",
    "\n",
    "2. **Prompt Design**:\n",
    "   - Designed prompts to generate **5 variations** of each query (Gpt-4o mini):\n",
    "     ```python\n",
    "     prompt = f\"\"\"Generate 5 different ways to ask this technical support question: {row['user_query']}, and your response be direct meaning give me directly the questions dont add number just sperate them with space\"\"\"\n",
    "     ```\n",
    "    - Designed prompts to generate **5 variations** of each query (Rogue Rose-103b-v0.2):\n",
    "        ```python\n",
    "        prompt = f\"\"\"Generate 5 different ways to ask this technical support question: {row['user_query']}\n",
    "        Important formatting rules:\n",
    "            - Each question must end with a question mark\n",
    "            - Questions should be written in a natural, conversational tone\n",
    "            - Do not include any numbers, bullets, or prefixes\n",
    "            - Separate questions only with question marks\n",
    "            - Return the questions as a continuous text without line breaks\"\"\"\n",
    "        ```\n",
    "\n",
    "##### **3.2 Execution**\n",
    "1. **GPT-4o Mini model**\n",
    "    1. **Data Processing**:\n",
    "        - Looped through the original dataset and generated variations for each query.\n",
    "        - Split multi-question outputs into individual queries.\n",
    "    2. **Performance**:\n",
    "        - Generated **13,798 rows** in **140 minutes**.\n",
    "        - Achieved a **13.5x increase** in dataset size.\n",
    "\n",
    "2. **Rogue Rose-103b-v0.2 model**\n",
    "    1. **Data Processing**:\n",
    "        - Looped through the original dataset and generated variations for each query.\n",
    "        - Remobe numbered bullets.\n",
    "    2. **Performance**:\n",
    "        - Generated **961 rows** in **39 minutes**.\n",
    "        - High-quality, diverse queries with different approch than GPT-4o Mini model\n",
    "##### **3.3 Results**\n",
    "- **Final Dataset Size**: 13,225 rows.\n",
    "- **Quality**: High-quality, diverse queries ready for training the chatbot.\n",
    "- **Diversity**: Generated queries covered a wide range of phrasings and scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Challenges and Solutions**\n",
    "| **Technique** | **Challenges** | **Solutions** |\n",
    "|------------------------|-----------------------------------------|--------------------------------------------|\n",
    "| Paraphrasing | Grammar errors, irrelevant content | Not used in final workflow \n",
    "| GPT-2 and T5 | irrelevant outputs, repetitive or nonsensical. | Not used in final workflow  |\n",
    "| Back-Translation | Did not significantly augment the data, generated slight variations of the original queries. | Combined with other augmented datasets to enhance diversity. |\n",
    "| GPT-4o Mini model | API rate limits, occasional errors | Error handling, retry logic |\n",
    "| Rogue Rose model| Numbered Bullets | retry logic |\n",
    "\n",
    "---\n",
    "#### **5. Translation Process**\n",
    "  - **Objective**: Translate the augmented dataset from English to French.\n",
    "  - **Tools Used**: `deep-translator` library\n",
    "  - **Results**:\n",
    "    1. **Version 1**:\n",
    "        - **Data included**: Original Data + Augmented Data by GPT-4o Mini\n",
    "        - Translate **13,225 rows** in **167 minutes**.\n",
    "        - We've got 3 dataset :\n",
    "            - English Version\n",
    "            - French Version\n",
    "            - Combination of two versions\n",
    "    1. **Version 2**:\n",
    "        - **Data included**: Version 1 + Augmented Data by Rogue-Rose-103b-v0.2 model \n",
    "        - Translate **14,165 rows** in **130 minutes**.\n",
    "        - We've got 3 dataset :\n",
    "            - English Version\n",
    "            - French Version\n",
    "            - Combination of two versions\n",
    "    1. **Version 3**:\n",
    "        - **Data included**: Version 2 + Back Translated Data\n",
    "        - Translate **14,952 rows** in **124 minutes**.\n",
    "        - We've got 3 dataset :\n",
    "            - English Version\n",
    "            - French Version\n",
    "            - Combination of two versions\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Dataset Statistics**\n",
    "| **Technique** | **Rows Generated** | **Time Taken** | **Used in Final Dataset** |\n",
    "|------------------------|---------------------|----------------|---------------------------|\n",
    "| Paraphrasing | 8,176 | 225 minutes | No |\n",
    "| Back-Translation | 1022 | 60 minutes | Yes |\n",
    "| GPT-4o Mini | 13,798 | 140 minutes | Yes |\n",
    "| Rogue Rose | 961 | 39 minutes | Yes |\n",
    "\n",
    "---\n",
    "\n",
    "#### **7. Final Data Workflow**\n",
    "| **Version** | **Data Included** | **Rows** |\n",
    "|------------------------|---------------------|----------------|\n",
    "| 1 | Original + GPT-4o mini Data Generated | 13,225 | \n",
    "| 2 | Original + GPT-4o mini Data Generated + Rogue Rose Data Generated | 14,165 | \n",
    "| 3 | Original + GPT-4o mini Data Generated + Rogue Rose Data Generated + Back Translated Data | 14952 |\n",
    "\n",
    "You will find all of them here : [Dataset](/Translation/data)\n",
    "\n",
    "---\n",
    "\n",
    "#### **8. Future Work**\n",
    "- **Fine-Tuning**: Fine-tune the chatbot model on the `3 versions augmented dataset` for better performance.\n",
    "- **Evaluation**: Evaluate the chatbot’s performance on real-world employee queries.\n",
    "- **Scaling**: Explore additional techniques (e.g., crowdsourcing) to further increase dataset size.\n",
    "\n",
    "---\n",
    "\n",
    "### *Installation*\n",
    "To install all required libraries, we create a requirements.txt file and to install them run this command:\n",
    "\n",
    "```bash\n",
    "    pip install -r requirements.txt\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
